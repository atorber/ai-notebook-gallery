{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIAK微调llama2-7b最佳实践\n",
    "\n",
    "本方案旨在帮助大模型开发者快速上手在百舸平台使用AIAK进行大模型训练，以llama2-7b为例演示如何使用AIAK镜像SFT大模型微调。\n",
    "\n",
    "## 步骤一：准备模型权重\n",
    "\n",
    "### 下载权重\n",
    "\n",
    "安装下载工具并下载HF权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://doc.bce.baidu.com/bce-documentation/BOS/linux-bcecmd-0.4.5.zip\n",
    "! apt-get install unzip\n",
    "! unzip linux-bcecmd-0.4.5.zip -d ./\n",
    "! ./linux-bcecmd-0.4.5/bcecmd bos sync bos:/cce-ai-datasets/huggingface.co/meta-llama/Llama-2-7b-hf /root/pfs/models/llama2-7b/hf/huggingface.co/meta-llama/Llama-2-7b-hf  --yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 权重格式转换\n",
    "权重格式转换，将权重从HF格式转换为MegatronCore格式，提交一个权重转换任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install python-dotenv\n",
    "! pip install bce-python-sdk-next==100.9.19.2\n",
    "! pip install future==1.0.0\n",
    "! pip install pycryptodome==3.20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "from baidubce.services.aihc.aihc_client import AIHCClient\n",
    "\n",
    "import baidubce.protocol\n",
    "from baidubce.bce_client_configuration import BceClientConfiguration\n",
    "from baidubce.auth.bce_credentials import BceCredentials\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 加载.env文件\n",
    "load_dotenv()\n",
    "\n",
    "# 获取配置信息, 从环境变量中获取,需要先在.env文件中设置环境变量\n",
    "ak = os.getenv(\"AK\")\n",
    "sk = os.getenv(\"SK\")\n",
    "host=os.getenv(\"HOST\")\n",
    "\n",
    "client_token = 'test-aihc-' + str(int(time.time()))\n",
    "logging.info('client_token: %s', client_token)\n",
    "\n",
    "config = BceClientConfiguration(\n",
    "    credentials=BceCredentials(ak, sk),\n",
    "    endpoint=host,\n",
    "    protocol=baidubce.protocol.HTTPS\n",
    ")\n",
    "\n",
    "aihc_client = AIHCClient(config)\n",
    "\n",
    "# 创建任务\n",
    "resourcePoolId = 'cce-e0isdmib'\n",
    "\n",
    "command = r\"\"\"#! /bin/bash\n",
    "\n",
    "AIAK_TRAINING_PATH=${AIAK_TRAINING_PATH:-\"/workspace/AIAK-Training-LLM\"}\n",
    "CONVERT_CHECKPOINT_PATH=\"$AIAK_TRAINING_PATH/tools/convert_checkpoint\"\n",
    "\n",
    "python $CONVERT_CHECKPOINT_PATH/model.py \\\n",
    "    --load_platform=huggingface \\\n",
    "    --save_platform=mcore \\\n",
    "    --common_config_path=$CONVERT_CHECKPOINT_PATH/config/llama2-7b.json \\\n",
    "    --tensor_model_parallel_size=${TP} \\\n",
    "    --pipeline_model_parallel_size=${PP} \\\n",
    "    --load_ckpt_path=$LOAD \\\n",
    "    --save_ckpt_path=$SAVE \\\n",
    "    --no_save_optim \\\n",
    "    --no_load_optim\"\"\"\n",
    "\n",
    "payload = {\n",
    "            \"name\": \"sft-llama2-7b-ck2mc-v1\",\n",
    "            \"jobSpec\": {\n",
    "                \"command\": command,\n",
    "                \"image\": \"registry.baidubce.com/aihc-aiak/aiak-training-llm:ubuntu22.04-cu12.3-torch2.2.0-py310-bccl1.2.7.2_v2.1.1.5_release\",\n",
    "                \"replicas\": 1,\n",
    "                \"envs\": [\n",
    "                    {\n",
    "                        \"name\": \"TP\",\n",
    "                        \"value\": \"1\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"PP\",\n",
    "                        \"value\": \"1\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"LOAD\",\n",
    "                        \"value\": \"/root/pfs/models/llama2-7b/hf/huggingface.co/meta-llama/Llama-2-7b-hf\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"SAVE\",\n",
    "                        \"value\": \"/root/pfs/models/llama2-7b/mcore/huggingface.co/meta-llama/Llama-2-7b-hf/tp1_pp1\"\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            \"datasources\": [\n",
    "                {\n",
    "                    \"type\": \"pfs\",\n",
    "                    \"name\": \"pfs-oYQuh4\",\n",
    "                    \"mountPath\": \"/root/pfs\"\n",
    "                }\n",
    "            ],\n",
    "            \"queue\": \"default\",\n",
    "            \"priority\": \"normal\",\n",
    "            \"jobFramework\": \"PyTorchJob\"\n",
    "        }\n",
    "res = aihc_client.create_aijob(\n",
    "    client_token=client_token,\n",
    "    resourcePoolId=resourcePoolId,\n",
    "    payload=payload\n",
    ")\n",
    "print('res', res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤二：准备预训练数据\n",
    "\n",
    "### 下载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget -o /root/pfs/datasets/WuDaoCorpus2.0_base_sample.tgz https://cce-ai-datasets.cdn.bcebos.com/datasets/aiak/WuDaoCorpus2.0_base_sample.tgz\n",
    "! tar zxvf WuDaoCorpus2.0_base_sample.tgz -C /root/pfs/datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据格式转换\n",
    "\n",
    "解压缩后得到10个JSON文件，文件内容为数组对象格式：\n",
    "\n",
    "```\n",
    "[\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"uniqueKey\": \"da73e2d0bb4e39d241c3806876621da7\",\n",
    "        \"titleUkey\": \"da73e2d0bb4e39d241c3806876621da7\",\n",
    "        \"dataType\": \"博客\",\n",
    "        \"title\": \"引网站蜘蛛的方法\",\n",
    "        \"content\": \"做站长的都希望自已做的网站被搜索引擎比如百度尽早收录。对于新站来说,蜘蛛可没不是呼之即来挥之即去的。 但是也不是一筹莫展,无计可施,只有摸透了这一只只神秘莫测的蜘蛛,有的放矢,才能随心所欲,对吧。呵呵。些话不多说,言归正传吧。 第一,蜘蛛的出动其实是非常讲究效率的,他们也懒得白跑,如果你的网站十天半个月不更新,他白跑几次后,也就不会来这么勤了。 所以,为了让蜘蛛天天来,那么就务必不让他空来,每次都喂点食。所以对策说就是最好每天更新内容了。 可以说,你规律性的多久更新一次,蜘蛛很可能也多久才来一次。 第二,尽量去掉网页上可有可无的部分吧,特别是java之类的,还有过大的图片,要尽量降低网页加载负荷,,加速网页的打开速度,网页打速度快,那么用户体验才好,跳出率才低,网页评分才高。 第三,检查内部链接结构,去除死链接和重复链接；死链接让蜘蛛原地打转,重复连接降低网页的新鲜度； 第四,尽量多从正规和相关站点获得反向链接,正规的链接能确保外链的稳定,以及免收株连；相关链接能提高外链的权重； 第五,为站点制作网站地图,包括格式和xml两种格式,作为蜘蛛爬行的向导,让蜘蛛能爬满整个网站而没有遗漏； 第六,确保服务器返回正确的数据包响应,这条比较玄,还不懂,你可以跟我说下什么意思； 第七、为每个页面制作独立的标题和meta标签(关键字、描述),这个在网页模板里写好调用代码就行； 第八、查看网页日志,监测蜘蛛的的爬行记录,蜘蛛爬行后会留下足迹,查看这些足迹就知道蜘蛛什么时候曾经光顾过这里了。 第九,直接快速的方法:用繁殖池自动繁殖引蜘蛛,它可以快速收录站群链接或者外推链接,可以实现亿万级蜘蛛网互联互串引蜘蛛,可以增加网站收录,提升网站排名。使用繁殖池引蜘蛛到其他平台发布外链不会受到种种限制。 需要繁殖池的联系官方qq: 咨询,马上为您申请开通！\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"uniqueKey\": \"0d78638721aecbdd798a072746114d20\",\n",
    "        \"titleUkey\": \"0d78638721aecbdd798a072746114d20\",\n",
    "        \"dataType\": \"博客\",\n",
    "        \"title\": \"千站云繁殖池收录神器是什么?\",\n",
    "        \"content\": \"千站云繁殖池是一款全新的升级收录算法,全面升级,6大功能,快速收录站群链接或者外推链接,已经取代于蜘蛛池,蜘蛛池的效果差已过时了； 那什么是繁殖池？适用于什么？有什么作用？ 繁殖池是大量网站将百度蜘蛛来访时集成一个池,通过程序控制自动繁殖外链地址给蜘蛛,这样可以快速大量收录站群链接或者外推链接； 适用于医疗媒体外推、站群、泛站、目录群、寄生虫、博客、微博、论坛、b2b信息,全自动繁殖不同地址引蜘蛛,实用而操作简单,效果好,可以实现亿万级蜘蛛网互联互串引蜘蛛,可以让新站、外推链接、媒体链接等等快速增加收录,被k网站也可以尽快恢复权重和搜索引擎快照,正常收录的网站可以增加网站收录,提升网站排名让你感受到不再需要为引蜘蛛到其他平台发布外链而受到种种限制等欢喜； 如果需要可以联系官方qq: 马上申请为你开通！！！\"\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "需要先将其处理为每行一个完整对象的jsonl格式：\n",
    "\n",
    "```\n",
    "{\"id\":1,\"uniqueKey\":\"da73e2d0bb4e39d241c3806876621da7\",\"titleUkey\":\"da73e2d0bb4e39d241c3806876621da7\",\"dataType\":\"博客\",\"title\":\"引网站蜘蛛的方法\",\"content\":\"做站长的都希望自已做的网站被搜索引擎比如百度尽早收录。对于新站来说,蜘蛛可没不是呼之即来挥之即去的。 但是也不是一筹莫展,无计可施,只有摸透了这一只只神秘莫测的蜘蛛,有的放矢,才能随心所欲,对吧。呵呵。些话不多说,言归正传吧。 第一,蜘蛛的出动其实是非常讲究效率的,他们也懒得白跑,如果你的网站十天半个月不更新,他白跑几次后,也就不会来这么勤了。 所以,为了让蜘蛛天天来,那么就务必不让他空来,每次都喂点食。所以对策说就是最好每天更新内容了。 可以说,你规律性的多久更新一次,蜘蛛很可能也多久才来一次。 第二,尽量去掉网页上可有可无的部分吧,特别是java之类的,还有过大的图片,要尽量降低网页加载负荷,,加速网页的打开速度,网页打速度快,那么用户体验才好,跳出率才低,网页评分才高。 第三,检查内部链接结构,去除死链接和重复链接；死链接让蜘蛛原地打转,重复连接降低网页的新鲜度； 第四,尽量多从正规和相关站点获得反向链接,正规的链接能确保外链的稳定,以及免收株连；相关链接能提高外链的权重； 第五,为站点制作网站地图,包括格式和xml两种格式,作为蜘蛛爬行的向导,让蜘蛛能爬满整个网站而没有遗漏； 第六,确保服务器返回正确的数据包响应,这条比较玄,还不懂,你可以跟我说下什么意思； 第七、为每个页面制作独立的标题和meta标签(关键字、描述),这个在网页模板里写好调用代码就行； 第八、查看网页日志,监测蜘蛛的的爬行记录,蜘蛛爬行后会留下足迹,查看这些足迹就知道蜘蛛什么时候曾经光顾过这里了。 第九,直接快速的方法:用繁殖池自动繁殖引蜘蛛,它可以快速收录站群链接或者外推链接,可以实现亿万级蜘蛛网互联互串引蜘蛛,可以增加网站收录,提升网站排名。使用繁殖池引蜘蛛到其他平台发布外链不会受到种种限制。 需要繁殖池的联系官方qq: 咨询,马上为您申请开通！\"}\n",
    "{\"id\":2,\"uniqueKey\":\"0d78638721aecbdd798a072746114d20\",\"titleUkey\":\"0d78638721aecbdd798a072746114d20\",\"dataType\":\"博客\",\"title\":\"千站云繁殖池收录神器是什么?\",\"content\":\"千站云繁殖池是一款全新的升级收录算法,全面升级,6大功能,快速收录站群链接或者外推链接,已经取代于蜘蛛池,蜘蛛池的效果差已过时了； 那什么是繁殖池？适用于什么？有什么作用？ 繁殖池是大量网站将百度蜘蛛来访时集成一个池,通过程序控制自动繁殖外链地址给蜘蛛,这样可以快速大量收录站群链接或者外推链接； 适用于医疗媒体外推、站群、泛站、目录群、寄生虫、博客、微博、论坛、b2b信息,全自动繁殖不同地址引蜘蛛,实用而操作简单,效果好,可以实现亿万级蜘蛛网互联互串引蜘蛛,可以让新站、外推链接、媒体链接等等快速增加收录,被k网站也可以尽快恢复权重和搜索引擎快照,正常收录的网站可以增加网站收录,提升网站排名让你感受到不再需要为引蜘蛛到其他平台发布外链而受到种种限制等欢喜； 如果需要可以联系官方qq: 马上申请为你开通！！！\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# 定义要保存的jsonl文件名\n",
    "output_file = '/root/pfs/datasets/datasets/aiak/WuDaoCorpus2.0_base_sample.jsonl'\n",
    "input_dir = '/root/pfs/datasets/WuDaoCorpus2.0_base_sample'\n",
    "# 打开文件准备写入\n",
    "with open(output_file, 'w') as file:\n",
    "    # 遍历当前目录下所有的.json文件\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith('.json'):\n",
    "            print(filename)\n",
    "            # 读取json文件内容\n",
    "            with open(input_dir+ '/' +filename, 'r') as json_file:\n",
    "                data_list = json.load(json_file)\n",
    "                # 遍历数据列表\n",
    "                for data in data_list:\n",
    "                    # 将字典转换为JSON字符串\n",
    "                    data['text'] = data['content']\n",
    "                    data['content'] = ''\n",
    "                    json_str = json.dumps(data)\n",
    "                    print(json_str)\n",
    "                    # 写入文件，并添加换行符\n",
    "                    file.write(json_str + '\\n')\n",
    "print(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 步骤二：使用AIAK提交训练任务\n",
    "\n",
    "### SFT微调模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "from baidubce.services.aihc.aihc_client import AIHCClient\n",
    "\n",
    "import baidubce.protocol\n",
    "from baidubce.bce_client_configuration import BceClientConfiguration\n",
    "from baidubce.auth.bce_credentials import BceCredentials\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 加载.env文件\n",
    "load_dotenv()\n",
    "\n",
    "# 获取配置信息, 从环境变量中获取,需要先在.env文件中设置环境变量\n",
    "ak = os.getenv(\"AK\")\n",
    "sk = os.getenv(\"SK\")\n",
    "host=os.getenv(\"HOST\")\n",
    "\n",
    "client_token = 'test-aihc-' + str(int(time.time()))\n",
    "logging.info('client_token: %s', client_token)\n",
    "\n",
    "config = BceClientConfiguration(\n",
    "    credentials=BceCredentials(ak, sk),\n",
    "    endpoint=host,\n",
    "    protocol=baidubce.protocol.HTTPS\n",
    ")\n",
    "\n",
    "aihc_client = AIHCClient(config)\n",
    "\n",
    "# 创建任务\n",
    "resourcePoolId = 'cce-e0isdmib'\n",
    "\n",
    "payload ={\n",
    "            \"queue\": \"default\",\n",
    "            \"priority\": \"normal\",\n",
    "            \"jobFramework\": \"PyTorchJob\",\n",
    "            \"name\": \"sft-llama2-7b-train-v1\",\n",
    "            \"jobSpec\": {\n",
    "                \"command\": \"bash /workspace/AIAK-Training-LLM/examples/llama2/finetuning/sft_llama2_7b.sh\",\n",
    "                \"image\": \"registry.baidubce.com/aihc-aiak/aiak-training-llm:ubuntu22.04-cu12.3-torch2.2.0-py310-bccl1.2.7.2_v2.1.1.5_release\",\n",
    "                \"replicas\": 1,\n",
    "                \"resources\": [\n",
    "                    {\n",
    "                        \"name\": \"baidu.com/a800_80g_cgpu\",\n",
    "                        \"quantity\": 8\n",
    "                    }\n",
    "                ],\n",
    "                \"enableRDMA\": True,\n",
    "                \"envs\": [\n",
    "                    {\n",
    "                        \"name\": \"CUDA_DEVICE_MAX_CONNECTIONS\",\n",
    "                        \"value\": \"1\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"DATA_PATH\",\n",
    "                        \"value\": \"/root/pfs/datasets/datasets/aiak/alpaca_zh-llama3-train.json\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"DATA_CACHE_PATH\",\n",
    "                        \"value\": \"/root/pfs/datasets/datasets/aiak/alpaca_zh-llama3-train_cache\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"TOKENIZER_PATH\",\n",
    "                        \"value\": \"/root/pfs/models/llama2-7b/hf/huggingface.co/meta-llama/Llama-2-7b-hf\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"CHECKPOINT_PATH\",\n",
    "                        \"value\": \"/root/pfs/models/llama2-7b/mcore/huggingface.co/meta-llama/Llama-2-7b-hf/tp1_pp1\"\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            \"datasources\": [\n",
    "                {\n",
    "                    \"type\": \"pfs\",\n",
    "                    \"name\": \"pfs-oYQuh4\",\n",
    "                    \"mountPath\": \"/root/pfs\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "res = aihc_client.create_aijob(\n",
    "    client_token=client_token,\n",
    "    resourcePoolId=resourcePoolId,\n",
    "    payload=payload\n",
    ")\n",
    "print('res', res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤三：模型转换为HF格式\n",
    "\n",
    "训练完成后需要将MegatronCore格式的模型转换为Huggingface格式，后续您可以使用转换后的Huggingface格式的模型进行服务在线部署。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "from baidubce.services.aihc.aihc_client import AIHCClient\n",
    "\n",
    "import baidubce.protocol\n",
    "from baidubce.bce_client_configuration import BceClientConfiguration\n",
    "from baidubce.auth.bce_credentials import BceCredentials\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 加载.env文件\n",
    "load_dotenv()\n",
    "\n",
    "# 获取配置信息, 从环境变量中获取,需要先在.env文件中设置环境变量\n",
    "ak = os.getenv(\"AK\")\n",
    "sk = os.getenv(\"SK\")\n",
    "host=os.getenv(\"HOST\")\n",
    "\n",
    "client_token = 'test-aihc-' + str(int(time.time()))\n",
    "logging.info('client_token: %s', client_token)\n",
    "\n",
    "config = BceClientConfiguration(\n",
    "    credentials=BceCredentials(ak, sk),\n",
    "    endpoint=host,\n",
    "    protocol=baidubce.protocol.HTTPS\n",
    ")\n",
    "\n",
    "aihc_client = AIHCClient(config)\n",
    "\n",
    "# 创建任务\n",
    "resourcePoolId = 'cce-e0isdmib'\n",
    "\n",
    "command = r\"\"\"#! /bin/bash\n",
    "\n",
    "AIAK_TRAINING_PATH=${AIAK_TRAINING_PATH:-\"/workspace/AIAK-Training-LLM\"}\n",
    "CONVERT_CHECKPOINT_PATH=\"$AIAK_TRAINING_PATH/tools/convert_checkpoint\"\n",
    "MEGATRON_PATH=${MEGATRON_PATH:-\"/workspace/AIAK-Megatron\"}\n",
    "\n",
    "python $CONVERT_CHECKPOINT_PATH/model.py \\\n",
    "    --load_platform=mcore \\\n",
    "    --save_platform=huggingface \\\n",
    "    --common_config_path=$CONVERT_CHECKPOINT_PATH/config/llama2-7b.json \\\n",
    "    --tensor_model_parallel_size=${TP} \\\n",
    "    --pipeline_model_parallel_size=${PP} \\\n",
    "    --megatron_path=$MEGATRON_PATH \\\n",
    "    --load_ckpt_path=$LOAD \\\n",
    "    --save_ckpt_path=$SAVE \\\n",
    "    --no_save_optim \\\n",
    "    --no_load_optim\"\"\"\n",
    "\n",
    "payload = {\n",
    "            \"name\": \"sft-llama2-7b-ck2hf-v1\",\n",
    "            \"jobSpec\": {\n",
    "                \"command\": command,\n",
    "                \"image\": \"registry.baidubce.com/aihc-aiak/aiak-training-llm:ubuntu22.04-cu12.3-torch2.2.0-py310-bccl1.2.7.2_v2.1.1.5_release\",\n",
    "                \"replicas\": 1,\n",
    "                \"envs\": [\n",
    "                    {\n",
    "                        \"name\": \"TP\",\n",
    "                        \"value\": \"1\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"PP\",\n",
    "                        \"value\": \"1\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"LOAD\",\n",
    "                        \"value\": \"/root/pfs/models/llama2-7b/mcore/huggingface.co/meta-llama/Llama-2-7b-hf/tp1_pp1/iter_0005000\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"SAVE\",\n",
    "                        \"value\": \"/root/pfs/models/llama2-7b/mcore/huggingface.co/meta-llama/Llama-2-7b-hf/tp1_pp1/iter_0005000_hf\"\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            \"datasources\": [\n",
    "                {\n",
    "                    \"type\": \"pfs\",\n",
    "                    \"name\": \"pfs-oYQuh4\",\n",
    "                    \"mountPath\": \"/root/pfs\"\n",
    "                }\n",
    "            ],\n",
    "            \"queue\": \"default\",\n",
    "            \"priority\": \"normal\",\n",
    "            \"jobFramework\": \"PyTorchJob\"\n",
    "        }\n",
    "res = aihc_client.create_aijob(\n",
    "    client_token=client_token,\n",
    "    resourcePoolId=resourcePoolId,\n",
    "    payload=payload\n",
    ")\n",
    "print('res', res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤四：部署及调用模型服务\n",
    "\n",
    "### 在控制台使用以下权重部署推理服务\n",
    "\n",
    "checkpoint在PFS中的存储位置\n",
    "\n",
    "```\n",
    "/models/llama2-7b/mcore/huggingface.co/meta-llama/Llama-2-7b-hf/tp1_pp1/iter_0005000_hf\n",
    "```\n",
    "\n",
    "### 调用推理服务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model_name = \"sft-llama2-7b\"  # 创建的服务名称，请求地址中的$MODEL_NAME\n",
    "    client = OpenAI(\n",
    "        # This is the default and can be omitted\n",
    "        api_key='aihc',\n",
    "        base_url=\"http://192.168.0.4/v1/model/sft-llama2-7b\",\n",
    "    )\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "#         temperature=0.8,\n",
    "#         top_p=0.8,\n",
    "#         n=1,\n",
    "        max_tokens=1024,\n",
    "#         frequency_penalty=1.5,\n",
    "#         stop=[\".\"],\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"},\n",
    "                  {'role': 'user', 'content': '你有什么能力？'}]\n",
    "\n",
    "    )\n",
    "    print(completion)\n",
    "    print(\"\\r\\n\")\n",
    "    for choice in completion.choices:\n",
    "        # 打印生成的文本,解码为中文\n",
    "        content = choice.message.content\n",
    "        print(content)\n",
    "        # print(\"[\" + choice.message.content + \"]\")\n",
    "        print(\"\\r\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "返回消息示例：\n",
    "\n",
    "```\n",
    "ChatCompletion(id='chatcmpl-5031ab8a-d323-4ed6-a323-2219b9a877c4', choices=[Choice(finish_reason='stop', index=None, logprobs=None, message=ChatCompletionMessage(content='\\n\\n答案：我是一个在社交媒体上友善并且积极的人。我会尽力帮助那些需要帮助的人，并且致力于在世界上创造积极的变化。<|im_end|></s>', refusal=None, role='assistant', function_call=None, tool_calls=None))], created='2024-09-09T22:06:11.399930621+08:00', model='sft-llama2-7b', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=53, prompt_tokens=60, total_tokens=113))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
